{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV7g81owlu4uJaGIbvzhYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RashmitVartak/DAV/blob/main/DAV_Exp7_63.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment - 11: Explore Data Visualization libraries in Python and R**"
      ],
      "metadata": {
        "id": "MTi1X99JTHZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:Rashmit Vartak\n",
        "\n",
        "Div: D11AD\n",
        "\n",
        "Rollno:63\n"
      ],
      "metadata": {
        "id": "3tQDcGypTHNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Title of Experiment**:  Perform the steps involved in Text Analytics in Python & R\n",
        "\n",
        "\n",
        "**Outcome of Experiment :**\n",
        "\n",
        "Design Text Analytics Application on a given data set. (LO4)\n",
        "\n",
        "**Tasks to be performed :**\n",
        "\n",
        "-Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "-Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "-Perform the following experiments using Python & R\n",
        "\n",
        "-Tokenization (Sentence & Word)\n",
        "\n",
        "-Frequency Distribution\n",
        "\n",
        "-Remove stopwords & punctuations\n",
        "\n",
        "-Lexicon Normalization (Stemming, Lemmatization)\n",
        "\n",
        "-Part of Speech tagging\n",
        "\n",
        "-Named Entity Recognization\n",
        "\n",
        "-Scrape data from a website\n",
        "\n",
        "**Theory :**\n",
        "\n",
        "1)Explore 5 Python Text Analytics Libraries in Python and write their features & applications.\n",
        "\n",
        "2)Explore 5 Python Text Analytics Libraries in R and write their features & applications.\n",
        "\n",
        "3)Steps involved in Text Analytics\n",
        "\n",
        "4)Benefits of Text Analytics"
      ],
      "metadata": {
        "id": "Pi8FWazsTHJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PYTHON"
      ],
      "metadata": {
        "id": "Eg6mJe4oVlbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZATION (Sentence and words)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"In the realm of artificial intelligence, the rapid advancements in language models have ushered in a new era of natural language processing. These sophisticated models, such as GPT-3, exhibit an unprecedented ability to understand and generate human-like text. With their vast knowledge base, these models empower users to interact with technology in a more intuitive and conversational manner. From generating creative writing to providing contextual responses, these language models have found applications in diverse fields, including content creation, customer support, and educational assistance. As they continue to evolve, the ethical considerations surrounding their usage and the potential societal impacts are becoming increasingly crucial topics of discussion. Balancing the transformative potential of these models with ethical guidelines and responsible deployment remains a key challenge for researchers, developers, and policymakers alike.\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n",
        "print(\"WordCount:\",len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC2b08B9VnXw",
        "outputId": "8e36a40d-568f-4804-a5f6-70328d68f166"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['In the realm of artificial intelligence, the rapid advancements in language models have ushered in a new era of natural language processing.', 'These sophisticated models, such as GPT-3, exhibit an unprecedented ability to understand and generate human-like text.', 'With their vast knowledge base, these models empower users to interact with technology in a more intuitive and conversational manner.', 'From generating creative writing to providing contextual responses, these language models have found applications in diverse fields, including content creation, customer support, and educational assistance.', 'As they continue to evolve, the ethical considerations surrounding their usage and the potential societal impacts are becoming increasingly crucial topics of discussion.', 'Balancing the transformative potential of these models with ethical guidelines and responsible deployment remains a key challenge for researchers, developers, and policymakers alike.']\n",
            "Words: ['In', 'the', 'realm', 'of', 'artificial', 'intelligence', ',', 'the', 'rapid', 'advancements', 'in', 'language', 'models', 'have', 'ushered', 'in', 'a', 'new', 'era', 'of', 'natural', 'language', 'processing', '.', 'These', 'sophisticated', 'models', ',', 'such', 'as', 'GPT-3', ',', 'exhibit', 'an', 'unprecedented', 'ability', 'to', 'understand', 'and', 'generate', 'human-like', 'text', '.', 'With', 'their', 'vast', 'knowledge', 'base', ',', 'these', 'models', 'empower', 'users', 'to', 'interact', 'with', 'technology', 'in', 'a', 'more', 'intuitive', 'and', 'conversational', 'manner', '.', 'From', 'generating', 'creative', 'writing', 'to', 'providing', 'contextual', 'responses', ',', 'these', 'language', 'models', 'have', 'found', 'applications', 'in', 'diverse', 'fields', ',', 'including', 'content', 'creation', ',', 'customer', 'support', ',', 'and', 'educational', 'assistance', '.', 'As', 'they', 'continue', 'to', 'evolve', ',', 'the', 'ethical', 'considerations', 'surrounding', 'their', 'usage', 'and', 'the', 'potential', 'societal', 'impacts', 'are', 'becoming', 'increasingly', 'crucial', 'topics', 'of', 'discussion', '.', 'Balancing', 'the', 'transformative', 'potential', 'of', 'these', 'models', 'with', 'ethical', 'guidelines', 'and', 'responsible', 'deployment', 'remains', 'a', 'key', 'challenge', 'for', 'researchers', ',', 'developers', ',', 'and', 'policymakers', 'alike', '.']\n",
            "WordCount: 146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#frequency distribution\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(words)\n",
        "most_common_words = word_counts.most_common(5)\n",
        "\n",
        "print(\"Most common words:\", most_common_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdnQfAIGVnyY",
        "outputId": "31471540-00cf-4cd1-b835-2436b39f68f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words: [(',', 11), ('.', 6), ('and', 6), ('the', 5), ('models', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing stopwords and punctutations\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiNDjH_jVnvw",
        "outputId": "05b2d506-2f89-408e-bdc2-b91d4e9b7555"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "stop_words.add(\"HENRY\")\n",
        "\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "filtered_words = [word for word in words if word not in stop_words and word not in punctuation]\n",
        "\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5AMoW7dVnsx",
        "outputId": "83f63208-1690-429f-f326-803919eba3cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words: ['In', 'realm', 'artificial', 'intelligence', 'rapid', 'advancements', 'language', 'models', 'ushered', 'new', 'era', 'natural', 'language', 'processing', 'These', 'sophisticated', 'models', 'GPT-3', 'exhibit', 'unprecedented', 'ability', 'understand', 'generate', 'human-like', 'text', 'With', 'vast', 'knowledge', 'base', 'models', 'empower', 'users', 'interact', 'technology', 'intuitive', 'conversational', 'manner', 'From', 'generating', 'creative', 'writing', 'providing', 'contextual', 'responses', 'language', 'models', 'found', 'applications', 'diverse', 'fields', 'including', 'content', 'creation', 'customer', 'support', 'educational', 'assistance', 'As', 'continue', 'evolve', 'ethical', 'considerations', 'surrounding', 'usage', 'potential', 'societal', 'impacts', 'becoming', 'increasingly', 'crucial', 'topics', 'discussion', 'Balancing', 'transformative', 'potential', 'models', 'ethical', 'guidelines', 'responsible', 'deployment', 'remains', 'key', 'challenge', 'researchers', 'developers', 'policymakers', 'alike']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsBkhjzMVnpw",
        "outputId": "20ce6429-50eb-4905-86d2-a12d9e6fd93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['in', 'realm', 'artifici', 'intellig', 'rapid', 'advanc', 'languag', 'model', 'usher', 'new', 'era', 'natur', 'languag', 'process', 'these', 'sophist', 'model', 'gpt-3', 'exhibit', 'unpreced', 'abil', 'understand', 'gener', 'human-lik', 'text', 'with', 'vast', 'knowledg', 'base', 'model', 'empow', 'user', 'interact', 'technolog', 'intuit', 'convers', 'manner', 'from', 'gener', 'creativ', 'write', 'provid', 'contextu', 'respons', 'languag', 'model', 'found', 'applic', 'divers', 'field', 'includ', 'content', 'creation', 'custom', 'support', 'educ', 'assist', 'as', 'continu', 'evolv', 'ethic', 'consider', 'surround', 'usag', 'potenti', 'societ', 'impact', 'becom', 'increasingli', 'crucial', 'topic', 'discuss', 'balanc', 'transform', 'potenti', 'model', 'ethic', 'guidelin', 'respons', 'deploy', 'remain', 'key', 'challeng', 'research', 'develop', 'policymak', 'alik']\n",
            "Lemmatized words: ['In', 'realm', 'artificial', 'intelligence', 'rapid', 'advancement', 'language', 'model', 'ushered', 'new', 'era', 'natural', 'language', 'processing', 'These', 'sophisticated', 'model', 'GPT-3', 'exhibit', 'unprecedented', 'ability', 'understand', 'generate', 'human-like', 'text', 'With', 'vast', 'knowledge', 'base', 'model', 'empower', 'user', 'interact', 'technology', 'intuitive', 'conversational', 'manner', 'From', 'generating', 'creative', 'writing', 'providing', 'contextual', 'response', 'language', 'model', 'found', 'application', 'diverse', 'field', 'including', 'content', 'creation', 'customer', 'support', 'educational', 'assistance', 'As', 'continue', 'evolve', 'ethical', 'consideration', 'surrounding', 'usage', 'potential', 'societal', 'impact', 'becoming', 'increasingly', 'crucial', 'topic', 'discussion', 'Balancing', 'transformative', 'potential', 'model', 'ethical', 'guideline', 'responsible', 'deployment', 'remains', 'key', 'challenge', 'researcher', 'developer', 'policymakers', 'alike']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#performing part of speeche recoginzation in the given input by  the user and distinguishing the words by determiner (DT), adjective (JJ), noun (NN), verb (VBZ), and preposition (IN).\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged_words = nltk.pos_tag(filtered_words)\n",
        "\n",
        "print(\"Part-of-speech tags:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6cgcRFdVnmy",
        "outputId": "8aca28da-1d8d-4994-ab10-1e53553fe83d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-speech tags: [('In', 'IN'), ('realm', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('rapid', 'JJ'), ('advancements', 'NNS'), ('language', 'NN'), ('models', 'NNS'), ('ushered', 'VBD'), ('new', 'JJ'), ('era', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('These', 'DT'), ('sophisticated', 'JJ'), ('models', 'NNS'), ('GPT-3', 'NNP'), ('exhibit', 'NN'), ('unprecedented', 'JJ'), ('ability', 'NN'), ('understand', 'VBP'), ('generate', 'NN'), ('human-like', 'NN'), ('text', 'NN'), ('With', 'IN'), ('vast', 'JJ'), ('knowledge', 'NN'), ('base', 'NN'), ('models', 'NNS'), ('empower', 'VBP'), ('users', 'NNS'), ('interact', 'JJ'), ('technology', 'NN'), ('intuitive', 'JJ'), ('conversational', 'JJ'), ('manner', 'NN'), ('From', 'IN'), ('generating', 'VBG'), ('creative', 'JJ'), ('writing', 'VBG'), ('providing', 'VBG'), ('contextual', 'JJ'), ('responses', 'NNS'), ('language', 'NN'), ('models', 'NNS'), ('found', 'VBN'), ('applications', 'NNS'), ('diverse', 'JJ'), ('fields', 'NNS'), ('including', 'VBG'), ('content', 'JJ'), ('creation', 'NN'), ('customer', 'NN'), ('support', 'NN'), ('educational', 'JJ'), ('assistance', 'NN'), ('As', 'IN'), ('continue', 'NN'), ('evolve', 'VBP'), ('ethical', 'JJ'), ('considerations', 'NNS'), ('surrounding', 'VBG'), ('usage', 'JJ'), ('potential', 'JJ'), ('societal', 'NN'), ('impacts', 'NNS'), ('becoming', 'VBG'), ('increasingly', 'RB'), ('crucial', 'JJ'), ('topics', 'NNS'), ('discussion', 'NN'), ('Balancing', 'NNP'), ('transformative', 'JJ'), ('potential', 'NN'), ('models', 'NNS'), ('ethical', 'JJ'), ('guidelines', 'NNS'), ('responsible', 'JJ'), ('deployment', 'NN'), ('remains', 'VBZ'), ('key', 'JJ'), ('challenge', 'NN'), ('researchers', 'NNS'), ('developers', 'NNS'), ('policymakers', 'NNS'), ('alike', 'RB')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install requests\n",
        "import requests\n",
        "r = requests.get('https://www.coursera.org/')"
      ],
      "metadata": {
        "id": "T8CwlhqDX3C7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTv6oBtFX26k",
        "outputId": "34476337-a111-4ab5-9948-27ffe5581030"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#R"
      ],
      "metadata": {
        "id": "KK6r6qCPVoUi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JuW9X6kLVqcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGhm75yRVqZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VfiyspgVqWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05w0M4mpVqS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrmhommXVqPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHsaJNn7VpII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}